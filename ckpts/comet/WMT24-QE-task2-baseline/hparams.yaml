activations: Tanh
batch_size: 4
class_identifier: unite_metric_multi_task
dropout: 0.1
encoder_learning_rate: 3.0e-06
encoder_model: XLM-RoBERTa
final_activation: null
hidden_sizes:
- 3072
- 1024
input_segments:
- mt
- src
keep_embeddings_frozen: true
layer: mix
layer_transformation: softmax
layerwise_decay: 0.95
learning_rate: 1.0e-05
loss: mse
loss_lambda: 0.5
nr_frozen_epochs: 0.3
optimizer: AdamW
pool: cls
pretrained_model: microsoft/infoxlm-large
qe_training: true
rnn_hidden_layers: 2
rnn_hidden_size: 1024
sampling_gamma: 1.0
sent_layer: mix
train_data:
- data/word-level-subtask/da/train_dev_test_mqm_lps_norm.csv
unite_training: false
use_rnn: false
validation_data:
- data/word-level-subtask/mqm/dev/en-de-dev.csv
- data/word-level-subtask/mqm/dev/en-ru-dev.csv
- data/word-level-subtask/mqm/dev/zh-en-dev.csv
word_layer: 24
word_level_training: true
word_weights:
- 0.1
- 0.9
