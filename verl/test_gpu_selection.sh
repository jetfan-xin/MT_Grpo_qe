#!/bin/bash

echo "ğŸ§ª æµ‹è¯•GPUé€‰æ‹©å’Œæ¨¡å‹åŠ è½½..."

# æµ‹è¯•GPUé€‰æ‹©è„šæœ¬
echo "1. æµ‹è¯•GPUé€‰æ‹©è„šæœ¬..."
./select_gpus.sh

echo ""
echo "2. æµ‹è¯•ç¯å¢ƒå˜é‡..."
echo "CUDA_VISIBLE_DEVICES: $CUDA_VISIBLE_DEVICES"
echo "SELECTED_GPUS: $SELECTED_GPUS"

echo ""
echo "3. æµ‹è¯•Pythonç¯å¢ƒ..."
python3 -c "
import torch
print(f'PyTorch version: {torch.__version__}')
print(f'CUDA available: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'CUDA device count: {torch.cuda.device_count()}')
    print(f'Current device: {torch.cuda.current_device()}')
    print(f'Device name: {torch.cuda.get_device_name()}')
    print(f'CUDA_VISIBLE_DEVICES: {torch.cuda.get_device_name() if torch.cuda.is_available() else \"N/A\"}')
"

echo ""
echo "4. æµ‹è¯•æ¨¡å‹åŠ è½½..."
python3 -c "
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

print('Loading tokenizer...')
tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2.5-3B', trust_remote_code=True)
print('âœ“ Tokenizer loaded successfully')

print('Loading model...')
model = AutoModelForCausalLM.from_pretrained(
    'Qwen/Qwen2.5-3B',
    torch_dtype=torch.bfloat16,
    attn_implementation='flash_attention_2',
    device_map='auto',
    trust_remote_code=True
)
print('âœ“ Model loaded successfully')

print(f'Model device: {next(model.parameters()).device}')
print(f'Model dtype: {next(model.parameters()).dtype}')
"

echo ""
echo "âœ… æµ‹è¯•å®Œæˆï¼"


